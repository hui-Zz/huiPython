# -*- coding:utf-8 -*-
import base64
import datetime
# import feedparser
import json
import os
import pymysql
import PyRSS2Gen
import re
import requests
import sys
import time
from configparser import ConfigParser
from curses.ascii import isdigit
from lxml import etree
from threading import Thread

# å•çº¿ç¨‹ï¼Œå¤šçº¿ç¨‹é‡‡é›†æ–¹å¼é€‰æ‹©(å¤šçº¿ç¨‹é‡‡é›†é€Ÿåº¦å¿«ä½†æœºå™¨è´Ÿè½½çŸ­æ—¶é«˜)
# thread = 'multi'
thread = 'single'
# é‡‡é›†æ•°æ®ä¿å­˜ç›®å½•,ä¸ºäº†å®‰å…¨è¯·ä¿®æ”¹æœ¬ç¨‹åºåå­—,æˆ–ç§»åŠ¨åˆ°å…¶ä»–ç›®å½•,å¹¶ä¿®æ”¹ä»¥ä¸‹è·¯å¾„,é»˜è®¤ä¸ç¨‹åºåŒç›®å½•
dir = os.path.dirname(os.path.abspath(__file__)) + "/json/"
iniPath = "huiNews.ini"
if(os.path.exists('/home/huinews/huiNews.ini')):
    iniPath = '/home/huinews/huiNews.ini'
config = ConfigParser()
config.read(iniPath)
rssPath = config.get('config', 'rss_path')
userAgent = config.get('hearders', 'User-Agent')
# ã€å¾®åšçƒ­æœã€‘
def parse_weibo(db):
    try:
        url = 'https://s.weibo.com/top/summary?cate=realtimehot'
        weiboCookie = config.get('hearders', 'weibo_cookie')
        hearders = {
            'User-Agent': userAgent,
            'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',
            'accept-encoding': 'gzip, deflate, br',
            'accept-language': 'zh-CN,zh;q=0.9,en-US;q=0.8,en;q=0.7',
            'cache-control': 'max-age=0',
            'cookie': weiboCookie,
            'referer': 'https://passport.weibo.com/'
        }
        hotTxt = requests.get(url, headers=hearders).content.decode()
        newHotTxt = hotTxt.replace('\n', '')
        newHotTxt = newHotTxt.replace(" ", "")
        noTdTxt = re.findall("<tbody>(.*?)</tbody>", newHotTxt)[0]
        arrayTxt = re.findall('ranktop">(.*?)<tdclass="td-03">', noTdTxt)
        # éå†æ•°æ®
        for x in range(5):
            ft = arrayTxt[x]
            # rank = arrayTxt[0:x.rfind("</")] #çƒ­æœæ’å
            urlTxt = ft.split('"')  # çƒ­æœé“¾æ¥
            hotName = ft.split(">")  # çƒ­æœåç§°
            title = re.sub(r'</a', "", hotName[3])
            span = re.sub(r'</span', "", hotName[5])
            label = re.sub(r'\d|\s', "", span)
            if label == 'ç»¼è‰º' or label == 'å‰§é›†' or label == 'ç”µå½±' or label == 'éŸ³ä¹':
                continue
            hot = re.sub(r'\D', "", span)
            emojis = re.findall(r"<imgsrc=\"(.+?)\"", ft)
            emoji = emojis[0] if emojis else ''
            contents = re.findall(r"title=\"(.+?)\"", ft)
            content = contents[0] if contents else ''
            # ä¿å­˜æ•°æ®
            db_insert('å¾®åš', 'çƒ­æœ', str(x + 1), title, 'https://s.weibo.com/' + urlTxt[3], hot, emoji, label, '', content)
        # æŸ¥è¯¢è¾“å‡º
        rssItems = db_query("å¾®åš")
        makeRss("å¾®åšçƒ­æœ", url, "å¾®åšçƒ­ç‚¹æ’è¡Œæ¦œ", "çƒ­æœ", rssItems)
    except Exception as e:
        print(sys._getframe().f_code.co_name+"é‡‡é›†é”™è¯¯ï¼Œè¯·åŠæ—¶æ›´æ–°è§„åˆ™ï¼" + str(e))


# ã€ç™¾åº¦çƒ­æœã€‘
def parse_baidu(db):
    try:
        url = 'https://top.baidu.com/board?platform=pc&sa=pcindex_a_right'
        hearders = {'User-Agent': userAgent}
        res = requests.get(url, headers=hearders)
        html = etree.HTML(res.content.decode())
        data = html.xpath('//*[@id="sanRoot"]/main/div[1]/div[1]/div[2]/a[*]/div[2]/div[2]/div/div/text()')
        linkList = html.xpath('//*[@id="sanRoot"]/main/div[1]/div[1]/div[2]/a/@href')
        coverList = html.xpath('//div[@class="active-item_1Em2h"]/img/@src')
        # éå†æ•°æ®
        for i, title in enumerate(data):
            if i > 4:
                break
            # ä¿å­˜æ•°æ®
            db_insert('ç™¾åº¦', 'çƒ­æœ', i, title.strip(), linkList[i], '', coverList[i], '', '', '')
        # æŸ¥è¯¢è¾“å‡º
        rssItems = db_query("ç™¾åº¦")
        makeRss("ç™¾åº¦çƒ­æœ", url, "ç™¾åº¦çƒ­æœé£äº‘æ¦œ", "çƒ­æœ", rssItems)
    except Exception as e:
        print(sys._getframe().f_code.co_name+"é‡‡é›†é”™è¯¯ï¼Œè¯·åŠæ—¶æ›´æ–°è§„åˆ™ï¼" + str(e))

# ã€çŸ¥ä¹çƒ­æ¦œã€‘
def parse_zhihu(db):
    try:
        url = 'https://www.zhihu.com/api/v3/feed/topstory/hot-lists/total?limit=50&desktop=true'
        headers = {'user-agent': userAgent}
        allResponse = requests.get(url, headers=headers).text
        jsonDecode = json.loads(allResponse)
        # éå†æ•°æ®
        for i in range(3):
            title = jsonDecode["data"][i]["target"]["title"]
            link = 'https://www.zhihu.com/question/' + str(jsonDecode["data"][i]["target"]["id"])
            cover = jsonDecode["data"][i]["children"][0]["thumbnail"]
            content = jsonDecode["data"][i]["target"]["excerpt"]
            hot = jsonDecode["data"][i]["detail_text"]
            # ä¿å­˜æ•°æ®
            db_insert('çŸ¥ä¹', '', str(i + 1), title, link, hot, cover, '', '', content)
        # æŸ¥è¯¢è¾“å‡º
        rssItems = db_query("çŸ¥ä¹")
        makeRss("çŸ¥ä¹çƒ­æ¦œ", url, "çŸ¥ä¹çƒ­é—¨æ’è¡Œæ¦œ", "", rssItems)
    except Exception as e:
        print(sys._getframe().f_code.co_name+"é‡‡é›†é”™è¯¯ï¼Œè¯·åŠæ—¶æ›´æ–°è§„åˆ™ï¼" + str(e))

# ã€Bç«™çƒ­æ¦œã€‘
def parse_bilibili(db):
    try:
        url = 'https://www.bilibili.com/v/popular/rank/all'
        hearders = {'User-Agent': userAgent}
        res = requests.get(url, headers=hearders)
        response = etree.HTML(res.content.decode())
        rank_lists = response.xpath('//ul[@class="rank-list"]/li')
        # è¯»å–å±è”½å…³é”®è¯
        blackTitle = config.get('black', 'title')
        blackAuthor = config.get('black', 'author')
        blackKeyword = config.get('black', 'keyword')
        blackGame = config.get('black', 'game')
        blackTitleList = blackTitle.split(',')
        blackAuthorList = blackAuthor.split(',')
        blackKeywordList = blackKeyword.split(',')
        blackGameList = blackGame.split(',')
        for rank_list in rank_lists:
            rank_num = rank_list.xpath('div/div/i/span/text()')
            if int(rank_num[0]) > 80:
                break
            title = rank_list.xpath('div/div[@class="info"]/a[@class="title"]/text()')
            link = rank_list.xpath('div/div[@class="info"]/a/@href')
            author = rank_list.xpath('div/div[@class="info"]/div[@class="detail"]/a/span/text()')
            hot = rank_list.xpath('div/div[@class="info"]/div[@class="detail"]/div/span[1]/text()')
            if author in blackAuthorList:
                continue
            if any(s in title for s in blackTitleList):
                continue
            bv = link[0].split('/video/')[-1]
            content = '<iframe src="https://player.bilibili.com/player.html?bvid=' + bv + \
                '&high_quality=1" width="650" height="477" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"></iframe>'
            # ä¿å­˜æ•°æ®
            db_insert('Bç«™', '', rank_num[0], title[0], 'https:' + link[0], hot[0].strip(), '', '', author[0].strip(), content)
        # è·å–è§†é¢‘å°é¢
        sql = "SELECT * FROM huinews \
            WHERE TO_DAYS( news_time ) = TO_DAYS(NOW()) \
            AND cover IS NULL AND source = %s" % ("'Bç«™'")
        try:
            # æ‰§è¡ŒSQLè¯­å¥
            cursor.execute(sql)
            # è·å–æ‰€æœ‰è®°å½•åˆ—è¡¨
            results = cursor.fetchall()
            rssItems = []
            for row in results:
                title = row[4]
                link = row[5]
                time.sleep(1)
                res = requests.get(link, headers=hearders)
                response = etree.HTML(res.content.decode())
                keywords = response.xpath('/html/head/meta[@name="keywords"]/@content')
                keyword = keywords[0].replace(title + ",", "")
                keyword = keyword.replace(",å“”å“©å“”å“©,Bilibili,Bç«™,å¼¹å¹•", "")
                keywordList = keyword.split(',')
                hui = 1
                if any(s in keyword for s in blackKeywordList):
                    hui = 0
                if 'æ¸¸æˆ' in keyword:
                    if any(s in keyword for s in blackGameList):
                        hui = 0
                description = response.xpath('/html/head/meta[@name="description"]/@content')
                imageUrl = response.xpath('/html/head/meta[@itemprop="image"]/@content')
                try:
                    update_re = "UPDATE huinews SET cover = '%s', label = '%s', hui = '%d', content=CONCAT(content,'%s') \
                                 WHERE link = '%s'" % (imageUrl[0], keyword, hui, description[0], link)
                    cursor.execute(update_re)
                    db.commit()
                except Exception as e:
                    db.rollback()
                    print(str(e))
        except Exception as e:
            print("æŸ¥è¯¢Bç«™æ— å°é¢è§†é¢‘å¤±è´¥ï¼" + str(e))
        # æŸ¥è¯¢è¾“å‡º
        rssItems = db_query("Bç«™")
        makeRss("Bç«™çƒ­æ¦œ", url, "Bç«™çƒ­é—¨æ’è¡Œæ¦œ", "", rssItems)
    except Exception as e:
        print(sys._getframe().f_code.co_name+"é‡‡é›†é”™è¯¯ï¼Œè¯·åŠæ—¶æ›´æ–°è§„åˆ™ï¼" + str(e))

# ã€ITä¹‹å®¶ã€‘
def parse_ithome(db):
    try:
        # å±è”½è¯
        blackIT = config.get('black', 'it')
        blackITList = blackIT.split(',')
        # [è·å–å°ç±³èµ„è®¯]
        url = 'https://m.ithome.com/search/%E5%B0%8F%E7%B1%B3.htm'
        hearders = {'User-Agent': userAgent}
        res = requests.get(url, headers=hearders)
        response = etree.HTML(res.content.decode())
        rank_lists = response.xpath('//div[@class="placeholder one-img-plc"]')
        for i, rank_list in enumerate(rank_lists):
            if i > 10:
                break
            title = rank_list.xpath('a/div[@class="plc-con"]/p[@class="plc-title"]/text()')
            link = rank_list.xpath('a/@href')
            cover = rank_list.xpath('a/div[@class="plc-image"]/img/@data-original')
            # ä¿å­˜æ•°æ®
            db_insert('ITä¹‹å®¶', '', str(i + 1), title[0], link[0], '', cover[0], 'å°ç±³', '', '')
        # [è·å–çƒ­æ¦œ]
        url = 'https://m.ithome.com/rankm'
        hearders = {'User-Agent': userAgent}
        res = requests.get(url, headers=hearders)
        response = etree.HTML(res.content.decode())
        rank_lists = response.xpath('//div[@class="placeholder one-img-plc"]')
        for i, rank_list in enumerate(rank_lists):
            if i > 10:
                break
            title = rank_list.xpath('a/div[@class="plc-con"]/p[@class="plc-title"]/text()')
            if any(s in title[0] for s in blackITList):
                continue
            link = rank_list.xpath('a/@href')
            cover = rank_list.xpath('a/div[@class="plc-image"]/img/@data-original')
            # ä¿å­˜æ•°æ®
            db_insert('ITä¹‹å®¶', '', str(i + 1), title[0], link[0], '', cover[0], '', '', '')
        # æŸ¥è¯¢è¾“å‡º
        rssItems = db_query("ITä¹‹å®¶")
        makeRss("ITä¹‹å®¶çƒ­æ¦œ", url, "ITä¹‹å®¶çƒ­æ¦œ", "", rssItems)
    except Exception as e:
        print(sys._getframe().f_code.co_name+"é‡‡é›†é”™è¯¯ï¼Œè¯·åŠæ—¶æ›´æ–°è§„åˆ™ï¼" + str(e))

# ã€ä»Šæ—¥çƒ­æ¦œ-æ¦œä¸­æ¦œã€‘
def parse_tophub(db):
    try:
        url = 'https://tophub.today/hot'
        tophubCookie = config.get('hearders', 'tophub_cookie')
        hearders = {'cookie': tophubCookie, 'User-Agent': userAgent}
        res = requests.get(url, headers=hearders)
        response = etree.HTML(res.content.decode())
        rank_lists = response.xpath('//div[@id="hotrank"]/div/div[@class="rank-section"][2]/ul/li[@class="child-item"]')
        for i, rank_list in enumerate(rank_lists):
            title = rank_list.xpath('div[@class="center-item"]/div/div/p[@class="medium-txt"]/a/text()')
            link = rank_list.xpath('div[@class="center-item"]/div/div/p[@class="medium-txt"]/a/@href')
            sourceStr = rank_list.xpath('div[@class="center-item"]/div/div/p[@class="small-txt"]/text()')
            sourceList = sourceStr[0].split(" â€§ ")
            # ä¿å­˜æ•°æ®
            db_insert(sourceList[0], '', str(i + 1), title[0], link[0], sourceList[1], '', 'æ¦œä¸­æ¦œ', '', '')
        # æŸ¥è¯¢è¾“å‡º
        rssItems = db_query("è™æ‰‘ç¤¾åŒº")
        makeRss("è™æ‰‘ç¤¾åŒº", url, "è™æ‰‘ç¤¾åŒº", "", rssItems)
    except Exception as e:
        print(sys._getframe().f_code.co_name+"é‡‡é›†é”™è¯¯ï¼Œè¯·åŠæ—¶æ›´æ–°è§„åˆ™ï¼" + str(e))

# ---------------------------------------------------------------------------------------------------------------------------------------------------

def db_query(name):
    # æŸ¥è¯¢æ•°æ®
    sql = "SELECT * FROM huinews \
        WHERE TO_DAYS( news_time ) = TO_DAYS(NOW()) \
        AND hui = 1 AND source = %s" % ("'" + name + "'")
    try:
        # æ‰§è¡ŒSQLè¯­å¥
        cursor.execute(sql)
        # è·å–æ‰€æœ‰è®°å½•åˆ—è¡¨
        results = cursor.fetchall()
        rssItems = []
        for row in results:
            source = row[1]
            rank = row[3]
            titleStr = row[4] + 'ğŸ”' if rank <= 1 else row[4]
            hot = ' ' + str(row[6]) if row[6] else ''
            times = ' x' + str(row[7])
            img = ' <img src="' + str(row[8]) + '" referrerpolicy="no-referrer"> ' if row[8] else ''
            label = ' ã€' + str(row[9]) + 'ã€' if row[9] else ''
            content = ' ' + str(row[11]) if row[11] else ''

            rssItem = PyRSS2Gen.RSSItem(
                title=titleStr if rank > 3 else titleStr + 'ğŸ”¥',
                link=row[5],
                description=str(rank) + times + label + hot + content + img,
                author = row[10],
                categories = row[2],
                pubDate=row[12]
            )
            rssItems.append(rssItem)
        return rssItems
    except Exception as e:
        print("æŸ¥è¯¢"+ name +"æ•°æ®å¤±è´¥ï¼" + str(e))

def db_insert(source,categories,rank,title,link,hot,cover,label,author,content):
    try:
        result = []
        result.append((source,categories,rank,title,link,hot,cover,label,author,content))
        # print(result)
        inesrt_re = "insert ignore into huinews (source,categories,rank,title,link,hot,cover,label,author,content) \
            values (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s) on duplicate key update times = times + 1"
        cursor = db.cursor()
        cursor.executemany(inesrt_re, result)
        db.commit()
    except Exception as e:
        db.rollback()
        print("æ’å…¥"+ source +"æ•°æ®å¤±è´¥ï¼" + str(e))

# æ‰“å¼€æ•°æ®åº“è¿æ¥
def db_connect():
    host = config.get('mysql', 'host')
    port = config.getint('mysql', 'port')
    user = config.get('mysql', 'user')
    password = config.get('mysql', 'password')
    database = config.get('mysql', 'database')
    db = pymysql.connect(host=host,
                         port=port,
                         user=user,
                         password=password,
                         database=database)
    return db

def makeRss(title, url, description, categories, rssItems):
    rss = PyRSS2Gen.RSS2(
        title=title,
        link=url,
        description=description,
        categories=categories,
        lastBuildDate=datetime.datetime.now(),
        items=rssItems)
    rss.write_xml(open(rssPath + title + '_Rss.xml', "w", encoding='utf-8'), encoding='utf-8')
    pass

# å­—ç¬¦æ›¿æ¢åŠ å¯†(é»˜è®¤ä¸ºå¤§å°å†™åè½¬),ä¿®æ”¹æ­¤å¤„é¡ºåºå’Œæ·»åŠ æ•°å­—æ›¿æ¢å¯å®ç°ä¸åŒå¯†ç åŠ å¯†(å¹¶åŒæ—¶ä¿®æ”¹get/index.phpå†…å¯†ç )
def multiple_replace(text):
    dic = {"a": "A", "b": "B", "c": "C", "d": "D", "e": "E", "f": "F", "g": "G", "h": "H", "i": "I", "j": "J", "k": "K", "l": "L", "m": "M", "n": "N", "o": "O", "p": "P", "q": "Q", "r": "R", "s": "S", "t": "T", "u": "U", "v": "V", "w": "W", "x": "X", "y": "Y", "z": "Z",
           "A": "a", "B": "b", "C": "c", "D": "d", "E": "e", "F": "f", "G": "g", "H": "h", "I": "i", "J": "j", "K": "k", "L": "l", "M": "m", "N": "n", "O": "o", "P": "p", "Q": "q", "R": "r", "S": "s", "T": "t", "U": "u", "V": "v", "W": "w", "X": "x", "Y": "y", "Z": "z"}
    pattern = "|".join(map(re.escape, list(dic.keys())))
    return re.sub(pattern, lambda m: dic[m.group()], text)

def single_run(db):
    # å•çº¿ç¨‹è¿è¡Œ
    print("å•çº¿ç¨‹é‡‡é›†å¼€å§‹", time.strftime("%Y-%m-%d %H:%M:%S", time.localtime()))
    t1 = time.time()
    parse_weibo(db)
    parse_baidu(db)
    parse_zhihu(db)
    parse_bilibili(db)
    parse_ithome(db)
    parse_tophub(db)
    print("å•çº¿ç¨‹é‡‡é›†å®Œæˆ", time.strftime("%Y-%m-%d %H:%M:%S", time.localtime()))
    print("è€—æ—¶:", time.time() - t1)

def multi_run(db):
    # å¤šçº¿ç¨‹æŠ“å–
    print("å¤šçº¿ç¨‹é‡‡é›†å¼€å§‹", time.strftime("%Y-%m-%d %H:%M:%S", time.localtime()))
    t1 = time.time()
    threads = []
    ts1 = Thread(target=parse_weibo, args=(db,))
    ts2 = Thread(target=parse_baidu, args=(db,))
    ts3 = Thread(target=parse_zhihu, args=(db,))
    threads.append(ts1)
    threads.append(ts2)
    threads.append(ts3)
    for t in threads:
        t.start()
    for t in threads:
        t.join()
    print("å¤šçº¿ç¨‹é‡‡é›†å®Œæˆ", time.strftime("%Y-%m-%d %H:%M:%S", time.localtime()))
    print("è€—æ—¶:", time.time() - t1)

if __name__ == "__main__":
    db = db_connect()
    # ä½¿ç”¨cursor()æ–¹æ³•è·å–æ“ä½œæ¸¸æ ‡
    cursor = db.cursor()

    if thread == 'single':
        # while True:
        single_run(db)
    if thread == 'multi':
        # while True:
        multi_run(db)

    # å…³é—­æ•°æ®åº“è¿æ¥
    db.close()
